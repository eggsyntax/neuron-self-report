{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Activation Projection Analysis\n",
    "\n",
    "This notebook analyzes the relationship between ground truth neuron activations and four projection directions:\n",
    "1. The prediction head direction\n",
    "2. The neuron input weight direction\n",
    "3. The neuron output weight direction\n",
    "4. The final residual stream\n",
    "\n",
    "Given an output directory from a previous run, the notebook will load model outputs and run a correlation analysis between these different directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from transformer_lens import HookedTransformer\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading data\n",
    "def load_head(model_path):\n",
    "    \"\"\"Load the saved prediction head and its configuration\"\"\"\n",
    "    config_path = os.path.join(model_path, \"config.json\")\n",
    "    head_path = os.path.join(model_path, \"head.pt\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    head_weights = torch.load(head_path, map_location=\"cpu\")\n",
    "    \n",
    "    return head_weights, config\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"Load dataset with input texts and ground truth activations\"\"\"\n",
    "    return pd.read_csv(dataset_path)\n",
    "\n",
    "def extract_model_info_from_path(output_dir):\n",
    "    \"\"\"Parse the output directory path to identify target layer and neuron\"\"\"\n",
    "    # Look for neuron information in directory name \n",
    "    dir_name = os.path.basename(output_dir)\n",
    "    # Try to extract from patterns like neuron_l8_n481\n",
    "    import re\n",
    "    matches = re.search(r'neuron_l(\\d+)_n(\\d+)', dir_name)\n",
    "    \n",
    "    if matches:\n",
    "        layer = int(matches.group(1))\n",
    "        neuron = int(matches.group(2))\n",
    "        return layer, neuron\n",
    "    \n",
    "    # If not found in directory name, look in subdirectories\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for d in dirs:\n",
    "            matches = re.search(r'neuron_l(\\d+)_n(\\d+)', d)\n",
    "            if matches:\n",
    "                layer = int(matches.group(1))\n",
    "                neuron = int(matches.group(2))\n",
    "                return layer, neuron\n",
    "    \n",
    "    # If still not found, look for a datasets directory\n",
    "    datasets_dir = os.path.join(output_dir, \"datasets\")\n",
    "    if os.path.exists(datasets_dir):\n",
    "        for file in os.listdir(datasets_dir):\n",
    "            if file.endswith(\".csv\"):\n",
    "                matches = re.search(r'neuron_l(\\d+)_n(\\d+)', file)\n",
    "                if matches:\n",
    "                    layer = int(matches.group(1))\n",
    "                    neuron = int(matches.group(2))\n",
    "                    return layer, neuron\n",
    "\n",
    "    # If still not found, check introspection summary\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            if \"introspection_summary\" in file and file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), \"r\") as f:\n",
    "                    try:\n",
    "                        summary = json.load(f)\n",
    "                        if \"target_layer\" in summary and \"target_neuron\" in summary:\n",
    "                            return summary[\"target_layer\"], summary[\"target_neuron\"]\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    # If we can't determine, raise an error\n",
    "    raise ValueError(f\"Could not determine layer and neuron from directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP weight access functions from linear_weights_analysis notebook\n",
    "def get_neuron_in_weights(model, layer, neuron=None):\n",
    "    \"\"\"Get weights from residual stream into MLP layer\"\"\"\n",
    "    # Try different model architectures\n",
    "    try:\n",
    "        # TransformerLens standard format\n",
    "        if hasattr(model.blocks[layer].mlp, \"W_in\"):\n",
    "            mlp_in = model.blocks[layer].mlp.W_in\n",
    "        # GPT-2 style\n",
    "        elif hasattr(model.blocks[layer].mlp, \"c_fc\"):\n",
    "            mlp_in = model.blocks[layer].mlp.c_fc\n",
    "        # Fallback for other architectures\n",
    "        else:\n",
    "            # Try to find any weight matrix in the MLP module\n",
    "            for name, module in model.blocks[layer].mlp.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear) and module.weight.shape[0] > model.cfg.d_model:\n",
    "                    mlp_in = module.weight\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"Couldn't find MLP input weights\")\n",
    "                \n",
    "        if neuron is not None:\n",
    "            return mlp_in[:, neuron]  # Column corresponding to this neuron\n",
    "        return mlp_in\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing MLP input weights: {e}\")\n",
    "        logging.error(\"Model structure:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if f\"blocks.{layer}\" in name and \"mlp\" in name and \"weight\" in name:\n",
    "                logging.error(f\"  {name}: {param.shape}\")\n",
    "        raise\n",
    "\n",
    "def get_neuron_out_weights(model, layer, neuron=None):\n",
    "    \"\"\"Get weights from MLP neuron to output\"\"\"\n",
    "    # Try different model architectures\n",
    "    try:\n",
    "        # TransformerLens standard format\n",
    "        if hasattr(model.blocks[layer].mlp, \"W_out\"):\n",
    "            mlp_out = model.blocks[layer].mlp.W_out\n",
    "        # GPT-2 style\n",
    "        elif hasattr(model.blocks[layer].mlp, \"c_proj\"):\n",
    "            mlp_out = model.blocks[layer].mlp.c_proj\n",
    "        # Fallback for other architectures\n",
    "        else:\n",
    "            # Try to find any weight matrix in the MLP module with appropriate shape\n",
    "            for name, module in model.blocks[layer].mlp.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear) and module.weight.shape[1] > model.cfg.d_model:\n",
    "                    mlp_out = module.weight\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"Couldn't find MLP output weights\")\n",
    "                \n",
    "        if neuron is not None:\n",
    "            return mlp_out[neuron] # Row corresponding to this neuron \n",
    "        return mlp_out\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing MLP output weights: {e}\")\n",
    "        logging.error(\"Model structure:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if f\"blocks.{layer}\" in name and \"mlp\" in name and \"weight\" in name:\n",
    "                logging.error(f\"  {name}: {param.shape}\")\n",
    "        raise\n",
    "\n",
    "def extract_linear_weights(head_weights, config, base_model=None):\n",
    "    \"\"\"Extract the linear prediction tensor from the head\"\"\"\n",
    "    head_type = config[\"head_type\"]\n",
    "    \n",
    "    if head_type == \"regression\":\n",
    "        # For regression head, extract the final linear layer weights\n",
    "        if \"hidden.weight\" in head_weights:\n",
    "            # Has hidden layer - need to compose the transformation\n",
    "            hidden_w = head_weights[\"hidden.weight\"]\n",
    "            hidden_b = head_weights[\"hidden.bias\"]\n",
    "            output_w = head_weights[\"output.weight\"]\n",
    "            output_b = head_weights[\"output.bias\"]\n",
    "            \n",
    "            # Note: This is an approximation, as we're ignoring the non-linearity\n",
    "            # We'll use only the linear component for direct comparison\n",
    "            linear_weights = output_w.squeeze()\n",
    "            logging.info(\"Using output layer weights from regression head with hidden layer\")\n",
    "        else:\n",
    "            # Direct linear projection\n",
    "            linear_weights = head_weights[\"output.weight\"].squeeze()\n",
    "            logging.info(\"Using output layer weights from regression head without hidden layer\")\n",
    "    \n",
    "    elif head_type == \"classification\":\n",
    "        # For classification head, combine weights based on bin edges\n",
    "        output_weights = head_weights[\"output.weight\"]\n",
    "        bin_edges = config.get(\"bin_edges\")\n",
    "        \n",
    "        if bin_edges:\n",
    "            # Calculate bin centers\n",
    "            bin_centers = [(bin_edges[i] + bin_edges[i+1])/2 for i in range(len(bin_edges)-1)]\n",
    "            \n",
    "            # Weight each class weight by its bin center to get a linear approximation\n",
    "            combined_weights = torch.zeros_like(output_weights[0])\n",
    "            for i, center in enumerate(bin_centers):\n",
    "                if i < output_weights.shape[0]:\n",
    "                    combined_weights += output_weights[i] * center\n",
    "            \n",
    "            linear_weights = combined_weights\n",
    "            logging.info(f\"Using weighted combination of classification weights with {len(bin_centers)} bin centers\")\n",
    "        else:\n",
    "            # If no bin edges, use the first class weight as an approximation\n",
    "            linear_weights = output_weights[0]\n",
    "            logging.warning(\"Using first class weight as approximation (no bin edges found)\")\n",
    "    \n",
    "    elif head_type == \"token\":\n",
    "        # For token head, weights come from the base model's embedding\n",
    "        if base_model is None:\n",
    "            raise ValueError(\"Base model is required for token head analysis\")\n",
    "            \n",
    "        # Get digit tokens (0-9) from the head config or use defaults\n",
    "        digit_tokens = config.get(\"head_config\", {}).get(\"digit_tokens\", list(range(48, 58)))  # ASCII 0-9\n",
    "        \n",
    "        if not digit_tokens:\n",
    "            # Fallback to ASCII digits if no tokens found\n",
    "            digit_tokens = list(range(48, 58))\n",
    "            \n",
    "        # Get embedding vectors for digit tokens\n",
    "        digit_embeddings = base_model.embed.weight[digit_tokens]\n",
    "        \n",
    "        # Weight each embedding by its value (0-9)\n",
    "        combined_weights = torch.zeros_like(digit_embeddings[0])\n",
    "        for i, embedding in enumerate(digit_embeddings):\n",
    "            combined_weights += embedding * (i / 9.0)  # Normalize to 0-1 range\n",
    "        \n",
    "        linear_weights = combined_weights\n",
    "        logging.info(f\"Using weighted combination of {len(digit_tokens)} token embeddings\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown head type: {head_type}\")\n",
    "    \n",
    "    return linear_weights.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Layer: 8, Target Neuron: 481\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory to analyze\n",
    "output_dir = \"/Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/important-outputs/unfrozen-last-layers-learn-to-predict-30-epochs/regression-after_target-500-30epochs-neuron_l8_n481--mse29.02-20250422_1259\"\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "# Extract target layer and neuron from directory name\n",
    "target_layer, target_neuron = extract_model_info_from_path(output_dir)\n",
    "print(f\"Target Layer: {target_layer}, Target Neuron: {target_neuron}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/mech_interp/notebooks\n",
      "Output dir: /Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/important-outputs/unfrozen-last-layers-learn-to-predict-30-epochs/regression-after_target-500-30epochs-neuron_l8_n481--mse29.02-20250422_1259\n",
      "Datasets dir: /Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/important-outputs/unfrozen-last-layers-learn-to-predict-30-epochs/regression-after_target-500-30epochs-neuron_l8_n481--mse29.02-20250422_1259/datasets\n",
      "File: neuron_l8_n481_20250422_120142_distribution.png\n",
      "File: neuron_l8_n481_20250422_120142.csv\n",
      "Loading dataset from: /Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/important-outputs/unfrozen-last-layers-learn-to-predict-30-epochs/regression-after_target-500-30epochs-neuron_l8_n481--mse29.02-20250422_1259/datasets/neuron_l8_n481_20250422_120142.csv\n",
      "Dataset shape: (500, 2)\n",
      "Dataset columns: ['text', 'activation']\n"
     ]
    }
   ],
   "source": [
    "# Find dataset file\n",
    "print(f'Working dir: {os.getcwd()}')\n",
    "print(f'Output dir: {output_dir}')\n",
    "datasets_dir = os.path.join(output_dir, \"datasets\")\n",
    "\n",
    "dataset_file = None\n",
    "print(f'Datasets dir: {datasets_dir}')\n",
    "if os.path.exists(datasets_dir):\n",
    "    for file in os.listdir(datasets_dir):\n",
    "        print(f'File: {file}')\n",
    "        if file.endswith(\".csv\") and f\"neuron_l{target_layer}_n{target_neuron}\" in file:\n",
    "            dataset_file = os.path.join(datasets_dir, file)\n",
    "            break\n",
    "else:\n",
    "    print(f'Nothing in datasets dir: {datasets_dir}')\n",
    "\n",
    "# Load the dataset\n",
    "if dataset_file and os.path.exists(dataset_file):\n",
    "    print(f\"Loading dataset from: {dataset_file}\")\n",
    "    dataset = load_dataset(dataset_file)\n",
    "    print(f\"Dataset shape: {dataset.shape}\")\n",
    "    print(f\"Dataset columns: {dataset.columns.tolist()}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset file not found for neuron l{target_layer}_n{target_neuron}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/egg/Documents/ai-safety/Research Projects/neuron_self_report/code1/important-outputs/unfrozen-last-layers-learn-to-predict-30-epochs/regression-after_target-500-30epochs-neuron_l8_n481--mse29.02-20250422_1259/models/neuron_l8_n481_20250422_120142/best_model\n",
      "Head type: regression\n"
     ]
    }
   ],
   "source": [
    "# Find model directory and load head\n",
    "models_dir = os.path.join(output_dir, \"models\")\n",
    "if not os.path.exists(models_dir):\n",
    "    # Look for models in parent directory structure\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        if \"models\" in dirs:\n",
    "            models_dir = os.path.join(root, \"models\")\n",
    "            break\n",
    "\n",
    "model_dir = None\n",
    "if os.path.exists(models_dir):\n",
    "    for d in os.listdir(models_dir):\n",
    "        if f\"neuron_l{target_layer}_n{target_neuron}\" in d:\n",
    "            # Look for best_model subdirectory\n",
    "            best_model_dir = os.path.join(models_dir, d, \"best_model\")\n",
    "            if os.path.exists(best_model_dir):\n",
    "                model_dir = best_model_dir\n",
    "                break\n",
    "            # Otherwise use the directory itself\n",
    "            model_dir = os.path.join(models_dir, d)\n",
    "            break\n",
    "\n",
    "if model_dir is None or not os.path.exists(model_dir):\n",
    "    # Look elsewhere\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        if \"best_model\" in dirs:\n",
    "            model_dir = os.path.join(root, \"best_model\")\n",
    "            break\n",
    "        for d in dirs:\n",
    "            if f\"neuron_l{target_layer}_n{target_neuron}\" in d and os.path.exists(os.path.join(root, d, \"config.json\")):\n",
    "                model_dir = os.path.join(root, d)\n",
    "                break\n",
    "        if model_dir:\n",
    "            break\n",
    "\n",
    "# Load the trained model head\n",
    "if model_dir and os.path.exists(model_dir):\n",
    "    print(f\"Loading model from: {model_dir}\")\n",
    "    head_weights, config = load_head(model_dir)\n",
    "    print(f\"Head type: {config['head_type']}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model directory not found for neuron l{target_layer}_n{target_neuron}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Base model: gpt2\n",
      "Model configuration:\n",
      "  Hidden dimension: 768\n",
      "  MLP dimension: 3072\n",
      "  Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "# Load the base transformer model\n",
    "base_model_name = config[\"base_model_name\"]\n",
    "base_model = HookedTransformer.from_pretrained(base_model_name)\n",
    "\n",
    "print(f\"Base model: {base_model_name}\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Hidden dimension: {base_model.cfg.d_model}\")\n",
    "print(f\"  MLP dimension: {base_model.cfg.d_mlp}\")\n",
    "print(f\"  Number of layers: {base_model.cfg.n_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Weight Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:36:50,667 - INFO - Using output layer weights from regression head without hidden layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head weights shape: torch.Size([768])\n",
      "Input weights shape: torch.Size([768])\n",
      "Output weights shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Extract the weight vectors for our analysis\n",
    "# 1. Linear prediction weights from the head\n",
    "head_weights_vector = extract_linear_weights(head_weights, config, base_model).to(device)\n",
    "print(f\"Head weights shape: {head_weights_vector.shape}\")\n",
    "\n",
    "# 2. MLP input weights for the target neuron\n",
    "input_weights_vector = get_neuron_in_weights(base_model, target_layer, target_neuron)\n",
    "print(f\"Input weights shape: {input_weights_vector.shape}\")\n",
    "\n",
    "# 3. MLP output weights for the target neuron\n",
    "output_weights_vector = get_neuron_out_weights(base_model, target_layer, target_neuron)\n",
    "print(f\"Output weights shape: {output_weights_vector.shape}\")\n",
    "\n",
    "# Create normalized versions for direction-only analysis\n",
    "head_weights_norm = F.normalize(head_weights_vector, dim=0)\n",
    "input_weights_norm = F.normalize(input_weights_vector, dim=0)\n",
    "output_weights_norm = F.normalize(output_weights_vector, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between head weights and input weights: 0.0453\n",
      "Cosine similarity between head weights and output weights: -0.0309\n",
      "Cosine similarity between input weights and output weights: -0.3627\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities between the weight vectors\n",
    "sim_head_input = F.cosine_similarity(head_weights_vector, input_weights_vector, dim=0)\n",
    "sim_head_output = F.cosine_similarity(head_weights_vector, output_weights_vector, dim=0)\n",
    "sim_input_output = F.cosine_similarity(input_weights_vector, output_weights_vector, dim=0)\n",
    "\n",
    "print(f\"Cosine similarity between head weights and input weights: {sim_head_input:.4f}\")\n",
    "print(f\"Cosine similarity between head weights and output weights: {sim_head_output:.4f}\")\n",
    "print(f\"Cosine similarity between input weights and output weights: {sim_input_output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Input Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hooks for collecting activations at different points\n",
    "pre_mlp_acts = []\n",
    "post_mlp_acts = []\n",
    "final_acts = []\n",
    "ground_truth_acts = []\n",
    "\n",
    "def pre_mlp_hook(acts, hook):\n",
    "    # Get the last token only\n",
    "    last_token_acts = acts[:, -1, :]\n",
    "    pre_mlp_acts.append(last_token_acts.detach().cpu())\n",
    "    return acts\n",
    "\n",
    "def post_mlp_hook(acts, hook):\n",
    "    # Get the last token only\n",
    "    last_token_acts = acts[:, -1, :]\n",
    "    post_mlp_acts.append(last_token_acts.detach().cpu())\n",
    "    return acts\n",
    "\n",
    "def final_layer_hook(acts, hook):\n",
    "    # Get the last token only\n",
    "    last_token_acts = acts[:, -1, :]\n",
    "    final_acts.append(last_token_acts.detach().cpu())\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 500 input texts\n",
      "Processing sample 0/500\n",
      "Processing sample 100/500\n",
      "Processing sample 200/500\n",
      "Processing sample 300/500\n",
      "Processing sample 400/500\n"
     ]
    }
   ],
   "source": [
    "# Process the input texts to collect activations\n",
    "# We'll sample a subset if the dataset is large\n",
    "max_samples = 500  # Adjust based on dataset size\n",
    "if len(dataset) > max_samples:\n",
    "    dataset_sample = dataset.sample(max_samples, random_state=42)\n",
    "else:\n",
    "    dataset_sample = dataset\n",
    "\n",
    "print(f\"Processing {len(dataset_sample)} input texts\")\n",
    "\n",
    "# Register hooks\n",
    "hooks = [\n",
    "    (f\"blocks.{target_layer}.hook_resid_pre\", pre_mlp_hook),\n",
    "    (f\"blocks.{target_layer}.hook_resid_post\", post_mlp_hook),\n",
    "    (f\"blocks.{base_model.cfg.n_layers-1}.hook_resid_post\", final_layer_hook)\n",
    "]\n",
    "\n",
    "# Set up results storage\n",
    "projection_results = []\n",
    "\n",
    "# Process inputs\n",
    "with torch.no_grad():\n",
    "    with base_model.hooks(hooks):\n",
    "        for i, row in dataset_sample.iterrows():\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing sample {i}/{len(dataset_sample)}\")\n",
    "                \n",
    "            # Get the input text and tokenize\n",
    "            input_text = row[\"text\"]\n",
    "            tokens = base_model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "            \n",
    "            # Run through the model\n",
    "            base_model(tokens[\"input_ids\"])\n",
    "            \n",
    "            # Get ground truth activation\n",
    "            ground_truth = row[\"activation\"]\n",
    "            ground_truth_acts.append(ground_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected mat.is_mps() to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Calculate input projections on pre-MLP activations\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m acts \u001b[38;5;129;01min\u001b[39;00m pre_mlp_acts:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Project pre-MLP activations onto input weights\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     proj = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_weights_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     input_projections.append(proj.item())\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Calculate output projections on post-MLP activations\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected mat.is_mps() to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "# Calculate projections onto the weight vectors\n",
    "head_projections = []\n",
    "input_projections = []\n",
    "output_projections = []\n",
    "neuron_dir_projections = []  # For the \"neuron direction\" in the final layer\n",
    "\n",
    "# Calculate head projections on final layer\n",
    "for acts in final_acts:\n",
    "    # Project final activations onto head weights\n",
    "    proj = torch.matmul(acts.to(device), head_weights_vector)\n",
    "    head_projections.append(proj.item())\n",
    "\n",
    "# Calculate input projections on pre-MLP activations\n",
    "for acts in pre_mlp_acts:\n",
    "    # Project pre-MLP activations onto input weights\n",
    "    proj = torch.matmul(acts.to(device), input_weights_vector)\n",
    "    input_projections.append(proj.item())\n",
    "    \n",
    "# Calculate output projections on post-MLP activations\n",
    "for acts in post_mlp_acts:\n",
    "    # Project post-MLP activations onto output weights\n",
    "    proj = torch.matmul(acts.to(device), output_weights_vector)\n",
    "    output_projections.append(proj.item())\n",
    "    \n",
    "# Calculate \"neuron direction\" projections on final layer\n",
    "# This is projecting the final layer activations onto the normalized output weights vector\n",
    "for acts in final_acts:\n",
    "    # Project final activations onto normalized output weights\n",
    "    proj = torch.matmul(acts.to(device), output_weights_norm)\n",
    "    neuron_dir_projections.append(proj.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results into a dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    \"ground_truth\": ground_truth_acts,\n",
    "    \"head_projection\": head_projections,\n",
    "    \"input_projection\": input_projections,\n",
    "    \"output_projection\": output_projections,\n",
    "    \"neuron_dir_projection\": neuron_dir_projections\n",
    "})\n",
    "\n",
    "# Display summary statistics\n",
    "print(results_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between ground truth and projections\n",
    "corr_head = results_df[\"ground_truth\"].corr(results_df[\"head_projection\"])\n",
    "corr_input = results_df[\"ground_truth\"].corr(results_df[\"input_projection\"])\n",
    "corr_output = results_df[\"ground_truth\"].corr(results_df[\"output_projection\"])\n",
    "corr_neuron_dir = results_df[\"ground_truth\"].corr(results_df[\"neuron_dir_projection\"])\n",
    "\n",
    "print(f\"Correlation between ground truth and head projection: {corr_head:.4f}\")\n",
    "print(f\"Correlation between ground truth and input projection: {corr_input:.4f}\")\n",
    "print(f\"Correlation between ground truth and output projection: {corr_output:.4f}\")\n",
    "print(f\"Correlation between ground truth and neuron direction projection: {corr_neuron_dir:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate full correlation matrix\n",
    "correlation_matrix = results_df.corr()\n",
    "print(\"Full correlation matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficient of determination (R²)\n",
    "r2_head = corr_head ** 2\n",
    "r2_input = corr_input ** 2\n",
    "r2_output = corr_output ** 2\n",
    "r2_neuron_dir = corr_neuron_dir ** 2\n",
    "\n",
    "print(f\"R² for head projection: {r2_head:.4f}\")\n",
    "print(f\"R² for input projection: {r2_input:.4f}\")\n",
    "print(f\"R² for output projection: {r2_output:.4f}\")\n",
    "print(f\"R² for neuron direction projection: {r2_neuron_dir:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistical significance\n",
    "n = len(results_df)\n",
    "\n",
    "def pearson_p_value(r, n):\n",
    "    # Calculate t statistic\n",
    "    t = r * np.sqrt(n - 2) / np.sqrt(1 - r**2)\n",
    "    # Calculate p-value\n",
    "    p = 2 * (1 - stats.t.cdf(abs(t), n - 2))\n",
    "    return p\n",
    "\n",
    "p_head = pearson_p_value(corr_head, n)\n",
    "p_input = pearson_p_value(corr_input, n)\n",
    "p_output = pearson_p_value(corr_output, n)\n",
    "p_neuron_dir = pearson_p_value(corr_neuron_dir, n)\n",
    "\n",
    "print(f\"p-value for head projection correlation: {p_head:.8f}\")\n",
    "print(f\"p-value for input projection correlation: {p_input:.8f}\")\n",
    "print(f\"p-value for output projection correlation: {p_output:.8f}\")\n",
    "print(f\"p-value for neuron direction projection correlation: {p_neuron_dir:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plots with regression lines and marginal histograms\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "\n",
    "# Create a 4-panel plot\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = gridspec.GridSpec(2, 2, figure=fig, wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Helper function to create scatter plot with regression line and histograms\n",
    "def create_scatter_with_marginals(x, y, xlabel, ylabel, ax_idx, corr, p_val, r2):\n",
    "    # Main scatter plot\n",
    "    ax_main = fig.add_subplot(gs[ax_idx])\n",
    "    ax_main.scatter(x, y, alpha=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    slope, intercept, _, _, _ = stats.linregress(x, y)\n",
    "    x_line = np.linspace(min(x), max(x), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "    ax_main.plot(x_line, y_line, 'r-')\n",
    "    \n",
    "    # Add correlation info\n",
    "    info_text = f\"r = {corr:.4f}\\np = {p_val:.4e}\\nR² = {r2:.4f}\"\n",
    "    ax_main.text(0.05, 0.95, info_text, transform=ax_main.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax_main.set_xlabel(xlabel)\n",
    "    ax_main.set_ylabel(ylabel)\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax_main\n",
    "\n",
    "# Create each panel\n",
    "ax1 = create_scatter_with_marginals(\n",
    "    results_df[\"head_projection\"], results_df[\"ground_truth\"], \n",
    "    \"Head Weight Projection\", \"Ground Truth Activation\", \n",
    "    0, corr_head, p_head, r2_head\n",
    ")\n",
    "ax1.set_title(\"Ground Truth vs. Head Weight Projection\")\n",
    "\n",
    "ax2 = create_scatter_with_marginals(\n",
    "    results_df[\"input_projection\"], results_df[\"ground_truth\"], \n",
    "    \"Input Weight Projection\", \"Ground Truth Activation\", \n",
    "    1, corr_input, p_input, r2_input\n",
    ")\n",
    "ax2.set_title(\"Ground Truth vs. Input Weight Projection\")\n",
    "\n",
    "ax3 = create_scatter_with_marginals(\n",
    "    results_df[\"output_projection\"], results_df[\"ground_truth\"], \n",
    "    \"Output Weight Projection\", \"Ground Truth Activation\", \n",
    "    2, corr_output, p_output, r2_output\n",
    ")\n",
    "ax3.set_title(\"Ground Truth vs. Output Weight Projection\")\n",
    "\n",
    "ax4 = create_scatter_with_marginals(\n",
    "    results_df[\"neuron_dir_projection\"], results_df[\"ground_truth\"], \n",
    "    \"Neuron Direction Projection\", \"Ground Truth Activation\", \n",
    "    3, corr_neuron_dir, p_neuron_dir, r2_neuron_dir\n",
    ")\n",
    "ax4.set_title(\"Ground Truth vs. Neuron Direction Projection\")\n",
    "\n",
    "# Main title for the figure\n",
    "plt.suptitle(f\"Projection Analysis for Neuron {target_neuron} in Layer {target_layer}\\n{base_model_name}\", \n",
    "             fontsize=16, y=0.98)\n",
    "\n",
    "plt.savefig(os.path.join(output_dir, f\"projection_analysis_l{target_layer}_n{target_neuron}.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.3f')\n",
    "plt.title(f\"Correlation Matrix for Projection Analysis\\nNeuron {target_neuron} in Layer {target_layer}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f\"projection_correlation_matrix_l{target_layer}_n{target_neuron}.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of findings\n",
    "summary = {\n",
    "    \"target_layer\": target_layer,\n",
    "    \"target_neuron\": target_neuron,\n",
    "    \"base_model\": base_model_name,\n",
    "    \"dataset_size\": len(results_df),\n",
    "    \"weight_similarities\": {\n",
    "        \"head_input\": float(sim_head_input),\n",
    "        \"head_output\": float(sim_head_output),\n",
    "        \"input_output\": float(sim_input_output)\n",
    "    },\n",
    "    \"projections\": {\n",
    "        \"head\": {\n",
    "            \"correlation\": float(corr_head),\n",
    "            \"r_squared\": float(r2_head),\n",
    "            \"p_value\": float(p_head)\n",
    "        },\n",
    "        \"input\": {\n",
    "            \"correlation\": float(corr_input),\n",
    "            \"r_squared\": float(r2_input),\n",
    "            \"p_value\": float(p_input)\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"correlation\": float(corr_output),\n",
    "            \"r_squared\": float(r2_output),\n",
    "            \"p_value\": float(p_output)\n",
    "        },\n",
    "        \"neuron_dir\": {\n",
    "            \"correlation\": float(corr_neuron_dir),\n",
    "            \"r_squared\": float(r2_neuron_dir),\n",
    "            \"p_value\": float(p_neuron_dir)\n",
    "        }\n",
    "    },\n",
    "    \"timestamp\": str(np.datetime64('now'))\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "summary_path = os.path.join(output_dir, f\"projection_analysis_results_l{target_layer}_n{target_neuron}.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary findings\n",
    "print(\"Summary of Projection Analysis:\\n\")\n",
    "print(f\"Target: Neuron {target_neuron} in Layer {target_layer} of {base_model_name}\")\n",
    "print(\"\\nWeight Direction Similarities:\")\n",
    "print(f\"  Head weights & Input weights: {sim_head_input:.4f}\")\n",
    "print(f\"  Head weights & Output weights: {sim_head_output:.4f}\")\n",
    "print(f\"  Input weights & Output weights: {sim_input_output:.4f}\")\n",
    "\n",
    "print(\"\\nProjection Correlations with Ground Truth:\")\n",
    "print(f\"  Head projection: r = {corr_head:.4f}, R² = {r2_head:.4f}, p = {p_head:.4e}\")\n",
    "print(f\"  Input projection: r = {corr_input:.4f}, R² = {r2_input:.4f}, p = {p_input:.4e}\")\n",
    "print(f\"  Output projection: r = {corr_output:.4f}, R² = {r2_output:.4f}, p = {p_output:.4e}\")\n",
    "print(f\"  Neuron direction projection: r = {corr_neuron_dir:.4f}, R² = {r2_neuron_dir:.4f}, p = {p_neuron_dir:.4e}\")\n",
    "\n",
    "# Determine which projection is most correlated with ground truth\n",
    "correlations = [\n",
    "    (\"Head projection\", abs(corr_head)),\n",
    "    (\"Input projection\", abs(corr_input)),\n",
    "    (\"Output projection\", abs(corr_output)),\n",
    "    (\"Neuron direction projection\", abs(corr_neuron_dir))\n",
    "]\n",
    "best_corr = max(correlations, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nThe projection most correlated with ground truth is: {best_corr[0]} (|r| = {best_corr[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "This analysis gives us insight into how different projections relate to the neuron's ground truth activation:\n",
    "\n",
    "1. **Head Weight Projection**: This shows how well the prediction head has learned to extract the neuron's activation from the final representation. A high correlation here indicates the predictor is working well.\n",
    "\n",
    "2. **Input Weight Projection**: This shows whether the residual stream immediately before the MLP layer already contains information about the neuron's activation, projected along its input weights. A high correlation would suggest the neuron is mostly passively reflecting existing information.\n",
    "\n",
    "3. **Output Weight Projection**: This shows how the neuron's activation is reflected back into the residual stream after the MLP layer. A high correlation could indicate that the neuron is \"writing\" its information clearly into the stream.\n",
    "\n",
    "4. **Neuron Direction Projection**: This shows if the neuron's activation persists in the final layer's representation along its initial direction. A high correlation suggests the neuron's information persists relatively unchanged throughout the network.\n",
    "\n",
    "These relationships help us understand how information flows through the model and which representation spaces best capture the neuron's behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
