{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Activation Patching\n",
    "\n",
    "This notebook explores the causal relationship between a neuron's activation and the model's prediction for that activation. By patching (modifying) the neuron's activation and observing changes in the prediction, we can determine whether the prediction model is directly using information from this neuron.\n",
    "\n",
    "This provides a causal complement to the correlational analysis in the linear weights notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load trained model head\n",
    "def load_head(model_path):\n",
    "    \"\"\"Load the saved prediction head and its configuration\"\"\"\n",
    "    config_path = os.path.join(model_path, \"config.json\")\n",
    "    head_path = os.path.join(model_path, \"head.pt\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    head_weights = torch.load(head_path, map_location=\"cpu\")\n",
    "    \n",
    "    return head_weights, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to a trained model - update this path to a valid model directory\n",
    "model_path = \"../output/models/neuron_l8_n481_20250422_173314\"\n",
    "\n",
    "# Load the trained model\n",
    "head_weights, config = load_head(model_path)\n",
    "\n",
    "# Display the config to understand what we're analyzing\n",
    "print(\"Model configuration:\")\n",
    "for key, value in config.items():\n",
    "    if key != \"head_config\":  # Skip printing the full head config for brevity\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Print head type details\n",
    "print(f\"\\nHead type: {config['head_type']}\")\n",
    "if \"head_config\" in config and \"hidden_dim\" in config[\"head_config\"]:\n",
    "    print(f\"Hidden dimension: {config['head_config']['hidden_dim']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base transformer model\n",
    "base_model_name = config[\"base_model_name\"]\n",
    "base_model = HookedTransformer.from_pretrained(base_model_name)\n",
    "\n",
    "# Fix for potential CUDA vs. MPS vs. CPU device issues\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "base_model.to(device)\n",
    "\n",
    "# Extract target neuron information\n",
    "target_layer = config[\"target_layer\"]\n",
    "target_neuron = config[\"target_neuron\"]\n",
    "layer_type = config[\"layer_type\"]\n",
    "print(f\"Analyzing neuron {target_neuron} in layer {target_layer}\")\n",
    "print(f\"Base model: {base_model_name}\")\n",
    "print(f\"Hidden dimension: {base_model.cfg.d_model}\")\n",
    "print(f\"MLP dimension: {base_model.cfg.d_mlp}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test inputs - diverse set of texts to test with\n",
    "test_inputs = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Machine learning models can be difficult to interpret.\",\n",
    "    \"Transformers use attention mechanisms to process sequences.\",\n",
    "    \"Neural networks have revolutionized artificial intelligence research.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Researchers work to make AI systems more transparent and explainable.\",\n",
    "    \"Scientists study the complex patterns in large language models.\",\n",
    "    \"Mechanistic interpretability aims to understand how neural networks work internally.\"\n",
    "]\n",
    "\n",
    "# Tokenize inputs\n",
    "tokenized_inputs = []\n",
    "for text in test_inputs:\n",
    "    tokens = base_model.tokenizer(text, return_tensors=\"pt\")\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}  # Move to device\n",
    "    tokenized_inputs.append(tokens)\n",
    "\n",
    "print(f\"Prepared {len(test_inputs)} test inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Patching and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with the trained head\n",
    "def predict_with_head(input_ids, attention_mask, head_weights, config, base_model):\n",
    "    \"\"\"Run the base model and predict with the trained head\"\"\"\n",
    "    # Run the base model to get activations\n",
    "    with torch.no_grad():\n",
    "        # Get output features at the appropriate layer\n",
    "        feature_layer = config.get(\"feature_layer\", -1)\n",
    "        outputs = base_model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "        # Get the feature representation (residual stream)\n",
    "        if feature_layer < 0:\n",
    "            feature_layer = base_model.cfg.n_layers + feature_layer\n",
    "        \n",
    "        # Extract features for the specified position\n",
    "        token_pos = config.get(\"token_pos\", \"last\")\n",
    "        if token_pos == \"last\":\n",
    "            pos = int((attention_mask.sum(-1) - 1)[0]) if attention_mask is not None else -1\n",
    "        else:\n",
    "            pos = int(token_pos)\n",
    "        \n",
    "        features = outputs.hidden_states[feature_layer][0, pos]\n",
    "        \n",
    "        # Apply the head\n",
    "        head_type = config[\"head_type\"]\n",
    "        \n",
    "        if head_type == \"regression\":\n",
    "            # Apply dropout, hidden layer (if present), and output layer\n",
    "            if \"hidden.weight\" in head_weights:\n",
    "                x = torch.nn.functional.dropout(features, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                x = torch.nn.functional.linear(x, head_weights[\"hidden.weight\"], head_weights[\"hidden.bias\"])\n",
    "                x = torch.nn.functional.gelu(x)\n",
    "                x = torch.nn.functional.dropout(x, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                pred = torch.nn.functional.linear(x, head_weights[\"output.weight\"], head_weights[\"output.bias\"])\n",
    "            else:\n",
    "                x = torch.nn.functional.dropout(features, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                pred = torch.nn.functional.linear(x, head_weights[\"output.weight\"], head_weights[\"output.bias\"])\n",
    "            \n",
    "            pred = pred.squeeze()\n",
    "            \n",
    "        elif head_type == \"classification\":\n",
    "            # Apply classification head\n",
    "            if \"hidden.weight\" in head_weights:\n",
    "                x = torch.nn.functional.dropout(features, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                x = torch.nn.functional.linear(x, head_weights[\"hidden.weight\"], head_weights[\"hidden.bias\"])\n",
    "                x = torch.nn.functional.gelu(x)\n",
    "                x = torch.nn.functional.dropout(x, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                logits = torch.nn.functional.linear(x, head_weights[\"output.weight\"], head_weights[\"output.bias\"])\n",
    "            else:\n",
    "                x = torch.nn.functional.dropout(features, p=config.get(\"head_config\", {}).get(\"dropout\", 0.1), training=False)\n",
    "                logits = torch.nn.functional.linear(x, head_weights[\"output.weight\"], head_weights[\"output.bias\"])\n",
    "            \n",
    "            # Convert to continuous prediction if bin edges are available\n",
    "            bin_edges = config.get(\"bin_edges\")\n",
    "            if bin_edges:\n",
    "                bin_centers = [(bin_edges[i] + bin_edges[i+1])/2 for i in range(len(bin_edges)-1)]\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Weight by bin centers\n",
    "                pred = 0\n",
    "                for i, center in enumerate(bin_centers):\n",
    "                    if i < probs.shape[-1]:\n",
    "                        pred += probs[i] * center\n",
    "            else:\n",
    "                pred = torch.argmax(logits).item()\n",
    "                \n",
    "        elif head_type == \"token\":\n",
    "            # Token prediction is handled differently\n",
    "            # This implementation needs to be customized based on your token head design\n",
    "            # Basic implementation shown here\n",
    "            logits = outputs.logits[0, pos]\n",
    "            digit_tokens = config.get(\"head_config\", {}).get(\"digit_tokens\", list(range(48, 58)))  # ASCII 0-9\n",
    "            \n",
    "            # Extract logits for digit tokens\n",
    "            digit_logits = torch.stack([logits[idx] for idx in digit_tokens])\n",
    "            probs = torch.nn.functional.softmax(digit_logits, dim=0)\n",
    "            \n",
    "            # Weight by digit values (0-9)\n",
    "            pred = 0\n",
    "            for i, p in enumerate(probs):\n",
    "                pred += i * p.item()\n",
    "            \n",
    "        return pred.item() if torch.is_tensor(pred) else pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the neuron activation\n",
    "def get_neuron_activation(input_ids, attention_mask, base_model, layer, neuron, layer_type=\"mlp_out\"):\n",
    "    \"\"\"Extract the activation of a specific neuron\"\"\"\n",
    "    with torch.no_grad():\n",
    "        _, cache = base_model.run_with_cache(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get last token position\n",
    "        pos = int((attention_mask.sum(-1) - 1)[0]) if attention_mask is not None else -1\n",
    "        \n",
    "        # Extract activation\n",
    "        activation = cache[layer_type, layer][0, pos, neuron].item()\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the actual activations\n",
    "example_input = tokenized_inputs[0]\n",
    "example_activation = get_neuron_activation(\n",
    "    example_input[\"input_ids\"],\n",
    "    example_input[\"attention_mask\"],\n",
    "    base_model,\n",
    "    target_layer,\n",
    "    target_neuron,\n",
    "    layer_type\n",
    ")\n",
    "\n",
    "print(f\"Neuron {target_neuron} activation for input '{test_inputs[0]}': {example_activation:.6f}\")\n",
    "\n",
    "# Also verify what's in the cache\n",
    "with torch.no_grad():\n",
    "    _, cache = base_model.run_with_cache(\n",
    "        example_input[\"input_ids\"],\n",
    "        attention_mask=example_input[\"attention_mask\"]\n",
    "    )\n",
    "    \n",
    "    # Print available cache keys\n",
    "    print(\"\\nCache keys (showing a subset):\")\n",
    "    for i, key in enumerate(list(cache.keys())[:10]):  # Show first 10 keys\n",
    "        print(f\"  {key}: {cache[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a patching hook\n",
    "def create_patching_hook(neuron_idx, new_value, patch_type=\"set\"):\n",
    "    \"\"\"Create a hook function for patching a neuron activation\"\"\"\n",
    "    def hook_fn(activation, hook):\n",
    "        # Get shape info\n",
    "        batch_size = activation.shape[0]\n",
    "        seq_len = activation.shape[1]\n",
    "        \n",
    "        # Create a copy to avoid modifying the original\n",
    "        patched = activation.clone()\n",
    "        \n",
    "        # Apply patching to the target neuron for all positions or just the last token\n",
    "        # We'll patch only the last token to match how we're extracting features\n",
    "        if hook.n_pos is not None:  # If we have position information\n",
    "            for i in range(batch_size):\n",
    "                pos = hook.n_pos[i]\n",
    "                if patch_type == \"set\":\n",
    "                    # Set to constant value\n",
    "                    patched[i, pos, neuron_idx] = new_value\n",
    "                elif patch_type == \"scale\":\n",
    "                    # Scale by factor\n",
    "                    patched[i, pos, neuron_idx] = activation[i, pos, neuron_idx] * new_value\n",
    "                elif patch_type == \"zero\":\n",
    "                    # Set to zero\n",
    "                    patched[i, pos, neuron_idx] = 0.0\n",
    "        else:  # If we don't have position info, assume last token\n",
    "            # This is simpler but maybe less precise\n",
    "            if patch_type == \"set\":\n",
    "                patched[:, -1, neuron_idx] = new_value\n",
    "            elif patch_type == \"scale\":\n",
    "                patched[:, -1, neuron_idx] = activation[:, -1, neuron_idx] * new_value\n",
    "            elif patch_type == \"zero\":\n",
    "                patched[:, -1, neuron_idx] = 0.0\n",
    "                \n",
    "        return patched\n",
    "    \n",
    "    return hook_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline neuron activations and predictions\n",
    "baseline_results = []\n",
    "\n",
    "for i, inputs in enumerate(tokenized_inputs):\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"] if \"attention_mask\" in inputs else None\n",
    "    \n",
    "    # Get neuron activation\n",
    "    activation = get_neuron_activation(\n",
    "        input_ids, \n",
    "        attention_mask, \n",
    "        base_model, \n",
    "        target_layer, \n",
    "        target_neuron, \n",
    "        layer_type\n",
    "    )\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = predict_with_head(\n",
    "        input_ids, \n",
    "        attention_mask, \n",
    "        head_weights, \n",
    "        config, \n",
    "        base_model\n",
    "    )\n",
    "    \n",
    "    baseline_results.append({\n",
    "        \"input\": test_inputs[i],\n",
    "        \"activation\": activation,\n",
    "        \"prediction\": prediction\n",
    "    })\n",
    "    \n",
    "    print(f\"Input {i+1}:\")\n",
    "    print(f\"  Text: {test_inputs[i][:50]}...\")\n",
    "    print(f\"  Activation: {activation:.4f}\")\n",
    "    print(f\"  Prediction: {prediction:.4f}\")\n",
    "\n",
    "# Calculate baseline statistics\n",
    "baseline_activations = [r[\"activation\"] for r in baseline_results]\n",
    "baseline_predictions = [r[\"prediction\"] for r in baseline_results]\n",
    "\n",
    "print(f\"\\nBaseline Statistics:\")\n",
    "print(f\"  Mean Activation: {np.mean(baseline_activations):.4f}\")\n",
    "print(f\"  Std Dev Activation: {np.std(baseline_activations):.4f}\")\n",
    "print(f\"  Mean Prediction: {np.mean(baseline_predictions):.4f}\")\n",
    "print(f\"  Std Dev Prediction: {np.std(baseline_predictions):.4f}\")\n",
    "print(f\"  Correlation: {np.corrcoef(baseline_activations, baseline_predictions)[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(baseline_activations, baseline_predictions, alpha=0.8, s=100)\n",
    "\n",
    "# Add text labels for each point\n",
    "for i, txt in enumerate(test_inputs):\n",
    "    # Truncate text for readability\n",
    "    short_txt = txt[:20] + \"...\" if len(txt) > 20 else txt\n",
    "    plt.annotate(short_txt, \n",
    "                 (baseline_activations[i], baseline_predictions[i]),\n",
    "                 fontsize=8,\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Add best fit line\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(baseline_activations, baseline_predictions)\n",
    "x = np.array([min(baseline_activations), max(baseline_activations)])\n",
    "plt.plot(x, slope * x + intercept, 'r--', \n",
    "         label=f'Linear fit (r={r_value:.2f}, p={p_value:.4f})')\n",
    "\n",
    "plt.title(f'Neuron {target_neuron} Activation vs. Prediction\\nCorrelation: {np.corrcoef(baseline_activations, baseline_predictions)[0,1]:.4f}')\n",
    "plt.xlabel('Neuron Activation')\n",
    "plt.ylabel('Model Prediction')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig('baseline_correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Patching Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run patching experiment with a constant value\n",
    "def run_patching_experiment(patch_value, patch_type=\"set\"):\n",
    "    \"\"\"Run experiment with patched neuron activation\"\"\"\n",
    "    patched_results = []\n",
    "    \n",
    "    # Create hook function\n",
    "    hook_fn = create_patching_hook(target_neuron, patch_value, patch_type)\n",
    "    \n",
    "    for i, inputs in enumerate(tokenized_inputs):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"] if \"attention_mask\" in inputs else None\n",
    "        \n",
    "        # Get last token position for hook context\n",
    "        last_pos = int((attention_mask.sum(-1) - 1)[0]) if attention_mask is not None else -1\n",
    "        \n",
    "        # Create a hook context object to pass to the hook\n",
    "        class HookContext:\n",
    "            def __init__(self, positions):\n",
    "                self.n_pos = positions\n",
    "                \n",
    "        hook_context = HookContext([last_pos])  # Pass position info to hook\n",
    "        \n",
    "        # Run with patching hook\n",
    "        hook_name = f\"{layer_type}.{target_layer}\"\n",
    "        with base_model.hooks([(hook_name, hook_fn)]):\n",
    "            # Get patched activation\n",
    "            patched_activation = get_neuron_activation(\n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                base_model, \n",
    "                target_layer, \n",
    "                target_neuron, \n",
    "                layer_type\n",
    "            )\n",
    "            \n",
    "            # Get prediction with patched activation\n",
    "            patched_prediction = predict_with_head(\n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                head_weights, \n",
    "                config, \n",
    "                base_model\n",
    "            )\n",
    "        \n",
    "        # Calculate changes from baseline\n",
    "        baseline = baseline_results[i]\n",
    "        act_change = patched_activation - baseline[\"activation\"]\n",
    "        pred_change = patched_prediction - baseline[\"prediction\"]\n",
    "        \n",
    "        patched_results.append({\n",
    "            \"input\": test_inputs[i],\n",
    "            \"patched_activation\": patched_activation,\n",
    "            \"patched_prediction\": patched_prediction,\n",
    "            \"activation_change\": act_change,\n",
    "            \"prediction_change\": pred_change\n",
    "        })\n",
    "    \n",
    "    return patched_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run constant-value patching experiment (setting activation to zero)\n",
    "constant_value = 0.0  # Try with zero\n",
    "constant_results = run_patching_experiment(constant_value, \"set\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Patching results (value={constant_value}):\")\n",
    "for i, result in enumerate(constant_results):\n",
    "    print(f\"Input {i+1}:\")\n",
    "    print(f\"  Baseline Activation: {baseline_results[i]['activation']:.4f}\")\n",
    "    print(f\"  Patched Activation: {result['patched_activation']:.4f}\")\n",
    "    print(f\"  Activation Change: {result['activation_change']:.4f}\")\n",
    "    print(f\"  Prediction Change: {result['prediction_change']:.4f}\")\n",
    "\n",
    "# Calculate average change\n",
    "avg_act_change = np.mean([r[\"activation_change\"] for r in constant_results])\n",
    "avg_pred_change = np.mean([r[\"prediction_change\"] for r in constant_results])\n",
    "print(f\"\\nAverage Activation Change: {avg_act_change:.4f}\")\n",
    "print(f\"Average Prediction Change: {avg_pred_change:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scaling experiment with multiple factors\n",
    "scale_factors = [0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.0]\n",
    "scaling_results = []\n",
    "\n",
    "for factor in scale_factors:\n",
    "    results = run_patching_experiment(factor, \"scale\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    avg_act_change = np.mean([r[\"activation_change\"] for r in results])\n",
    "    avg_pred_change = np.mean([r[\"prediction_change\"] for r in results])\n",
    "    \n",
    "    scaling_results.append({\n",
    "        \"factor\": factor,\n",
    "        \"results\": results,\n",
    "        \"avg_act_change\": avg_act_change,\n",
    "        \"avg_pred_change\": avg_pred_change\n",
    "    })\n",
    "    \n",
    "    print(f\"Scale factor {factor}:\")\n",
    "    print(f\"  Avg Activation Change: {avg_act_change:.4f}\")\n",
    "    print(f\"  Avg Prediction Change: {avg_pred_change:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling experiment results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot activation changes\n",
    "act_changes = [r[\"avg_act_change\"] for r in scaling_results]\n",
    "pred_changes = [r[\"avg_pred_change\"] for r in scaling_results]\n",
    "factors = [r[\"factor\"] for r in scaling_results]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(factors, act_changes, 'b-o', label='Activation Change')\n",
    "plt.plot(factors, pred_changes, 'r-o', label='Prediction Change')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=1.0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.title(f'Effect of Scaling Neuron {target_neuron}')\n",
    "plt.xlabel('Scaling Factor')\n",
    "plt.ylabel('Change from Baseline')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot changes in predicted vs. actual space\n",
    "plt.subplot(1, 2, 2)\n",
    "# Get baseline means\n",
    "baseline_act_mean = np.mean(baseline_activations)\n",
    "baseline_pred_mean = np.mean(baseline_predictions)\n",
    "\n",
    "# Calculate projected values after scaling\n",
    "scaled_acts = [baseline_act_mean + change for change in act_changes]\n",
    "scaled_preds = [baseline_pred_mean + change for change in pred_changes]\n",
    "\n",
    "# Plot with connecting lines in activation-prediction space\n",
    "plt.plot(scaled_acts, scaled_preds, 'g-o')\n",
    "plt.plot([baseline_act_mean], [baseline_pred_mean], 'ko', markersize=10, label='Baseline (factor=1.0)')\n",
    "\n",
    "# Add labels for scaling factors\n",
    "for i, factor in enumerate(factors):\n",
    "    plt.annotate(f\"{factor:.2f}\", \n",
    "                 (scaled_acts[i], scaled_preds[i]),\n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.title('Activation-Prediction Space')\n",
    "plt.xlabel('Average Activation')\n",
    "plt.ylabel('Average Prediction')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scaling_experiment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis of Individual Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if prediction changes are proportional to activation changes\n",
    "pred_vs_act = []\n",
    "for result in scaling_results:\n",
    "    for sample in result[\"results\"]:\n",
    "        pred_vs_act.append({\n",
    "            \"input\": sample[\"input\"],\n",
    "            \"act_change\": sample[\"activation_change\"],\n",
    "            \"pred_change\": sample[\"prediction_change\"],\n",
    "            \"factor\": result[\"factor\"]\n",
    "        })\n",
    "\n",
    "# Plot prediction change vs activation change\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(scale_factors)))\n",
    "\n",
    "# Create a plot with all individual points\n",
    "for i, factor in enumerate(scale_factors):\n",
    "    # Get points for this scaling factor\n",
    "    points = [p for p in pred_vs_act if p[\"factor\"] == factor]\n",
    "    plt.scatter(\n",
    "        [p[\"act_change\"] for p in points],\n",
    "        [p[\"pred_change\"] for p in points],\n",
    "        label=f'Scale={factor}',\n",
    "        color=colors[i],\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Prediction Change vs Activation Change\\nAll Inputs and Scaling Factors')\n",
    "plt.xlabel('Activation Change')\n",
    "plt.ylabel('Prediction Change')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pred_vs_act_change_all.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model to quantify the relationship\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data\n",
    "X = np.array([p[\"act_change\"] for p in pred_vs_act]).reshape(-1, 1)\n",
    "y = np.array([p[\"pred_change\"] for p in pred_vs_act])\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = model.score(X, y)\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Plot the data with regression line\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X, y, alpha=0.6)\n",
    "plt.plot(\n",
    "    [X.min(), X.max()], \n",
    "    [model.predict([[X.min()]])[0], model.predict([[X.max()]])[0]], \n",
    "    'r-', linewidth=2\n",
    ")\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title(f'Linear Relationship between Activation and Prediction Changes\\nSlope: {slope:.4f}, R²: {r2:.4f}')\n",
    "plt.xlabel('Activation Change')\n",
    "plt.ylabel('Prediction Change')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add equation on the plot\n",
    "equation = f\"y = {slope:.4f}x + {intercept:.4f}\"\n",
    "plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "             backgroundcolor='white', fontsize=12)\n",
    "\n",
    "plt.savefig('pred_vs_act_regression.png')\n",
    "plt.show()\n",
    "\n",
    "# Print out the relationship summary\n",
    "print(f\"Linear Relationship Summary:\")\n",
    "print(f\"  Slope: {slope:.6f}\")\n",
    "print(f\"  Intercept: {intercept:.6f}\")\n",
    "print(f\"  R² coefficient: {r2:.6f}\")\n",
    "print(f\"  Equation: Prediction Change = {slope:.4f} × Activation Change + {intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-input analysis to see which inputs are most affected\n",
    "per_input_sensitivity = {}\n",
    "\n",
    "# Organize data by input\n",
    "for inp in test_inputs:\n",
    "    points = [p for p in pred_vs_act if p[\"input\"] == inp]\n",
    "    \n",
    "    if len(points) >= 2:  # Need at least 2 points for regression\n",
    "        X_inp = np.array([p[\"act_change\"] for p in points]).reshape(-1, 1)\n",
    "        y_inp = np.array([p[\"pred_change\"] for p in points])\n",
    "        \n",
    "        # Fit individual model\n",
    "        inp_model = LinearRegression()\n",
    "        inp_model.fit(X_inp, y_inp)\n",
    "        \n",
    "        # Store results\n",
    "        per_input_sensitivity[inp] = {\n",
    "            \"slope\": float(inp_model.coef_[0]),\n",
    "            \"r2\": inp_model.score(X_inp, y_inp),\n",
    "            \"points\": len(points),\n",
    "            \"X\": X_inp.flatten().tolist(),\n",
    "            \"y\": y_inp.tolist()\n",
    "        }\n",
    "\n",
    "# Sort inputs by sensitivity (slope)\n",
    "sorted_inputs = sorted(per_input_sensitivity.items(), \n",
    "                       key=lambda x: abs(x[1][\"slope\"]), \n",
    "                       reverse=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Per-input sensitivity analysis:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Input':<50} | {'Slope':>10} | {'R²':>10} | {'Points':>6}\")\n",
    "print(\"-\" * 80)\n",
    "for inp, data in sorted_inputs:\n",
    "    # Truncate input text\n",
    "    short_inp = inp[:47] + \"...\" if len(inp) > 47 else inp\n",
    "    print(f\"{short_inp:<50} | {data['slope']:>10.4f} | {data['r2']:>10.4f} | {data['points']:>6}\")\n",
    "\n",
    "# Visualize individual input models\n",
    "plt.figure(figsize=(15, 12))\n",
    "n_inputs = len(sorted_inputs)\n",
    "rows = (n_inputs + 1) // 2  # Calculate rows needed\n",
    "\n",
    "for i, (inp, data) in enumerate(sorted_inputs[:min(n_inputs, 8)]):  # Show at most 8 inputs\n",
    "    plt.subplot(rows, 2, i+1)\n",
    "    \n",
    "    # Plot points\n",
    "    plt.scatter(data[\"X\"], data[\"y\"], alpha=0.7)\n",
    "    \n",
    "    # Plot regression line\n",
    "    x_range = np.array([min(data[\"X\"]), max(data[\"X\"])])\n",
    "    slope = data[\"slope\"]\n",
    "    intercept = 0  # Assuming zero intercept for simplicity\n",
    "    plt.plot(x_range, slope * x_range + intercept, 'r-')\n",
    "    \n",
    "    # Add info\n",
    "    short_title = inp[:30] + \"...\" if len(inp) > 30 else inp\n",
    "    plt.title(f\"{short_title}\\nSlope: {slope:.4f}, R²: {data['r2']:.4f}\")\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('per_input_sensitivity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of our findings\n",
    "summary = {\n",
    "    \"model_path\": model_path,\n",
    "    \"head_type\": config[\"head_type\"],\n",
    "    \"target_layer\": target_layer,\n",
    "    \"target_neuron\": target_neuron,\n",
    "    \"baseline_correlation\": float(np.corrcoef(baseline_activations, baseline_predictions)[0,1]),\n",
    "    \"slope\": float(slope),\n",
    "    \"intercept\": float(intercept),\n",
    "    \"r_squared\": float(r2),\n",
    "    \"scaling_factors\": scale_factors,\n",
    "    \"avg_activation_changes\": [float(r[\"avg_act_change\"]) for r in scaling_results],\n",
    "    \"avg_prediction_changes\": [float(r[\"avg_pred_change\"]) for r in scaling_results],\n",
    "    \"per_input_sensitivity\": {k: {\n",
    "        \"slope\": v[\"slope\"],\n",
    "        \"r2\": v[\"r2\"],\n",
    "        \"points\": v[\"points\"]\n",
    "    } for k, v in per_input_sensitivity.items()},\n",
    "    \"timestamp\": str(np.datetime64('now'))\n",
    "}\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "results_path = f\"../results/activation_patching_{target_layer}_{target_neuron}.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "\n",
    "Based on the patching experiments, we can make the following interpretations:\n",
    "\n",
    "1. **Causal Relationship**: \n",
    "   - The slope of the regression line (`slope`) indicates how much the prediction changes when the neuron's activation changes. A non-zero slope indicates the prediction is causally affected by this neuron's activation.\n",
    "   - The R² value (`r_squared`) tells us how much of the prediction's variation is explained by changes in this neuron's activation.\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - If `slope` is close to 1.0: The predictor is directly tracking this neuron's activation\n",
    "   - If `slope` is close to 0.0: The predictor ignores this neuron\n",
    "   - If `slope` is negative: The predictor is inversely related to this neuron's activation\n",
    "   - If `r_squared` is high (close to 1.0): The relationship is strong and consistent\n",
    "   - If `r_squared` is low (close to 0.0): The relationship is weak or inconsistent\n",
    "\n",
    "3. **Per-Input Variation**:\n",
    "   - Different inputs show different sensitivities to neuron patching\n",
    "   - This variation might reveal context-dependent computation\n",
    "   - Inputs with high sensitivity slopes are most influenced by this neuron\n",
    "\n",
    "4. **Comparison with Weight Analysis**:\n",
    "   - If the linear weights notebook showed low similarity but patching shows high sensitivity, this suggests the predictor uses this neuron's information but encodes it differently\n",
    "   - If both analyses show strong relationships, we have stronger evidence that the predictor is directly modeling this neuron\n",
    "\n",
    "This causal analysis complements the correlational approach from the weights comparison, giving us a more complete picture of how the predictor relates to the target neuron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}